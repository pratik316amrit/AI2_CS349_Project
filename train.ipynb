{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  # Import gymnasium instead of gym\n",
    "from stable_baselines3 import DQN\n",
    "import ale_py\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "# Initialize and wrap the environment\n",
    "env = gym.make(\"ALE/Pong-v5\",render_mode=None)\n",
    "\n",
    "# Convert action space to Discrete (since Pong has 6 actions but we want only one discrete action)\n",
    "# For Pong, typically you would reduce the multi-discrete action to one action: 0 (stay) or 1 (move up), etc.\n",
    "env = gym.wrappers.GrayScaleObservation(env)  # Convert the observations to grayscale for efficiency\n",
    "env = gym.wrappers.ResizeObservation(env, 84)  # Resize for faster training\n",
    "env = gym.wrappers.FrameStack(env, 4)  # Stack frames (a common practice in RL)\n",
    "\n",
    "# Convert the environment to a vectorized environment (required by SB3)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Example Bayesian function that adjusts action probabilities\n",
    "def bayesian_action_probability(action_success_rate, current_performance):\n",
    "    # Assume a normal distribution for simplicity\n",
    "    probability = norm.cdf(current_performance, loc=action_success_rate, scale=0.1)\n",
    "    return probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "\n",
    "# Define fuzzy variables\n",
    "state_quality = ctrl.Antecedent(np.arange(0, 11, 1), 'state_quality')\n",
    "action_intensity = ctrl.Consequent(np.arange(0, 11, 1), 'action_intensity')\n",
    "\n",
    "# Define fuzzy sets and rules\n",
    "state_quality['poor'] = fuzz.trimf(state_quality.universe, [0, 0, 5])\n",
    "state_quality['average'] = fuzz.trimf(state_quality.universe, [0, 5, 10])\n",
    "state_quality['good'] = fuzz.trimf(state_quality.universe, [5, 10, 10])\n",
    "\n",
    "action_intensity['low'] = fuzz.trimf(action_intensity.universe, [0, 0, 5])\n",
    "action_intensity['medium'] = fuzz.trimf(action_intensity.universe, [0, 5, 10])\n",
    "action_intensity['high'] = fuzz.trimf(action_intensity.universe, [5, 10, 10])\n",
    "\n",
    "# Define fuzzy rules\n",
    "rule1 = ctrl.Rule(state_quality['poor'], action_intensity['low'])\n",
    "rule2 = ctrl.Rule(state_quality['average'], action_intensity['medium'])\n",
    "rule3 = ctrl.Rule(state_quality['good'], action_intensity['high'])\n",
    "\n",
    "# Control system\n",
    "action_ctrl = ctrl.ControlSystem([rule1, rule2, rule3])\n",
    "action_decision = ctrl.ControlSystemSimulation(action_ctrl)\n",
    "\n",
    "# Example fuzzy decision\n",
    "def fuzzy_action_decision(state_quality_value):\n",
    "    action_decision.input['state_quality'] = state_quality_value\n",
    "    action_decision.compute()\n",
    "    return action_decision.output['action_intensity']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaAgent:\n",
    "    def __init__(self):\n",
    "        self.exploration_rate = 0.5  # adjust based on performance\n",
    "        self.fuzzy_weight = 0.5  # weight for fuzzy decisions\n",
    "        self.bayesian_weight = 0.5  # weight for Bayesian decisions\n",
    "\n",
    "    def adjust_weights(self, performance):\n",
    "        if performance > 0.7:\n",
    "            self.exploration_rate *= 0.9  # reduce exploration as agent performs well\n",
    "        else:\n",
    "            self.exploration_rate *= 1.1  # increase exploration if performance is low\n",
    "\n",
    "        # Adjust weights dynamically\n",
    "        self.fuzzy_weight = max(0.3, self.fuzzy_weight - 0.05 * (performance - 0.5))\n",
    "        self.bayesian_weight = max(0.3, self.bayesian_weight + 0.05 * (0.5 - performance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 191      |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3144     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0113   |\n",
      "|    n_updates        | 760      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 191      |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6521     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0035   |\n",
      "|    n_updates        | 1605     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 191      |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9847     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00773  |\n",
      "|    n_updates        | 2436     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 204      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3577     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000198 |\n",
      "|    n_updates        | 3344     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 196      |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 7172     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00367  |\n",
      "|    n_updates        | 4242     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 209      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3601     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000164 |\n",
      "|    n_updates        | 5825     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 154      |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 7492     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00022  |\n",
      "|    n_updates        | 6797     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 93       |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 3209     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000167 |\n",
      "|    n_updates        | 8202     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 87       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 6635     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00407  |\n",
      "|    n_updates        | 9058     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 94       |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 3604     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000208 |\n",
      "|    n_updates        | 10775    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 88       |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 7173     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000342 |\n",
      "|    n_updates        | 11668    |\n",
      "----------------------------------\n",
      "Episode 0 - Score: [-2.]\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MetaAgent and the DQN model\n",
    "meta_agent = MetaAgent()\n",
    "model = DQN(\"CnnPolicy\", env, verbose=1, device=\"cuda\", buffer_size=50000, batch_size=128)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Extract the first observation from the batch\n",
    "        obs_single = obs[0]  # Unwrap the first observation\n",
    "\n",
    "        # Extract state quality (simplified here as random)\n",
    "        state_quality_value = np.random.uniform(0, 10)\n",
    "        \n",
    "        # Fuzzy action decision\n",
    "        fuzzy_action = fuzzy_action_decision(state_quality_value)\n",
    "        \n",
    "        # Bayesian decision\n",
    "        bayesian_prob = bayesian_action_probability(0.6, score / (episode + 1))\n",
    "        \n",
    "        # Meta-agent adjusts exploration rate and weights\n",
    "        meta_agent.adjust_weights(score / (episode + 1))\n",
    "        \n",
    "        # Combine fuzzy and Bayesian decisions\n",
    "        action_value = fuzzy_action * meta_agent.fuzzy_weight + bayesian_prob * meta_agent.bayesian_weight\n",
    "\n",
    "        # Ensure that the result is a scalar before converting to int\n",
    "        action = int(action_value.item())  # .item() ensures it's a scalar before converting\n",
    "        action = model.predict(obs_single, deterministic=action > meta_agent.exploration_rate)[0]  # Use the unwrapped observation\n",
    "        \n",
    "        # The action needs to be passed as a batch\n",
    "        action_batch = [action]\n",
    "\n",
    "        # Take action in the environment\n",
    "        obs, reward, done, info = env.step(action_batch)\n",
    "        score += reward\n",
    "\n",
    "        # Train the model\n",
    "        model.learn(total_timesteps=10000)\n",
    "\n",
    "    print(f\"Episode {episode} - Score: {score}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"dqn_pong_model\")\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
